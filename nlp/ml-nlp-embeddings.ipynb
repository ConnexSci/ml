{"cells":[{"cell_type":"code","source":"!pip3 install datasets\n\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport pickle \nimport os\nimport sys\n\n# from torch_geometric.data import Data, DataLoader\n# from torch_geometric.nn import GCNConv, GATConv, SAGEConv, global_mean_pool\n# from torch_geometric.utils import to_networkx\n\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\n\nimport datasets\n\nROOT = os.getcwd().replace('ml/nlp', '')\nprint(ROOT)\n\ndataset = datasets.load_dataset('ncbi_disease')\ndf_ncbi = pd.DataFrame(dataset['train'])\n\ndf_ncbi.head()","metadata":{"cell_id":"480d05616b304dfe9b25e45907dc384b","source_hash":"832de250","execution_start":1666547899361,"execution_millis":11767,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /root/venv/lib/python3.9/site-packages (2.6.1)\nRequirement already satisfied: fsspec[http]>=2021.11.1 in /root/venv/lib/python3.9/site-packages (from datasets) (2022.10.0)\nRequirement already satisfied: packaging in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from datasets) (21.3)\nRequirement already satisfied: responses<0.19 in /root/venv/lib/python3.9/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: aiohttp in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from datasets) (3.8.1)\nRequirement already satisfied: requests>=2.19.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from datasets) (2.28.1)\nRequirement already satisfied: tqdm>=4.62.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from datasets) (4.64.1)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /root/venv/lib/python3.9/site-packages (from datasets) (0.10.1)\nRequirement already satisfied: xxhash in /root/venv/lib/python3.9/site-packages (from datasets) (3.1.0)\nRequirement already satisfied: pyyaml>=5.1 in /root/venv/lib/python3.9/site-packages (from datasets) (6.0)\nRequirement already satisfied: multiprocess in /root/venv/lib/python3.9/site-packages (from datasets) (0.70.13)\nRequirement already satisfied: dill<0.3.6 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from datasets) (0.3.5.1)\nRequirement already satisfied: pyarrow>=6.0.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from datasets) (9.0.0)\nRequirement already satisfied: numpy>=1.17 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from datasets) (1.23.3)\nRequirement already satisfied: pandas in /root/venv/lib/python3.9/site-packages (from datasets) (1.5.1)\nRequirement already satisfied: yarl<2.0,>=1.0 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from aiohttp->datasets) (1.8.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from aiohttp->datasets) (2.1.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from aiohttp->datasets) (1.2.0)\nRequirement already satisfied: attrs>=17.3.0 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from aiohttp->datasets) (22.1.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.3.0)\nRequirement already satisfied: filelock in /shared-libs/python3.9/py/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: certifi>=2017.4.17 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2022.6.15.1)\nRequirement already satisfied: idna<4,>=2.5 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.12)\nRequirement already satisfied: pytz>=2020.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from pandas->datasets) (2022.2.1)\nRequirement already satisfied: python-dateutil>=2.8.1 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: six>=1.5 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0m/shared-libs/python3.9/py/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n/work/\nDownloading builder script: 100%|██████████| 5.83k/5.83k [00:00<00:00, 4.41MB/s]\nDownloading metadata: 100%|██████████| 3.45k/3.45k [00:00<00:00, 3.97MB/s]\nDownloading readme: 100%|██████████| 9.27k/9.27k [00:00<00:00, 6.78MB/s]\nDownloading and preparing dataset ncbi_disease/ncbi_disease (download: 1.47 MiB, generated: 3.04 MiB, post-processed: Unknown size, total: 4.52 MiB) to /root/.cache/huggingface/datasets/ncbi_disease/ncbi_disease/1.0.0/92314c7992b0b8a5ea2ad101be33f365b684a2cc011e0ffa29c691e6d32b2d03...\nDownloading data files:   0%|          | 0/3 [00:00<?, ?it/s]\nDownloading data: 1.14MB [00:00, 64.4MB/s]                  \nDownloading data files:  33%|███▎      | 1/3 [00:00<00:01,  1.80it/s]\nDownloading data: 200kB [00:00, 35.5MB/s]                    \nDownloading data files:  67%|██████▋   | 2/3 [00:00<00:00,  2.69it/s]\nDownloading data: 206kB [00:00, 35.6MB/s]                    \nDownloading data files: 100%|██████████| 3/3 [00:01<00:00,  2.61it/s]\nExtracting data files: 100%|██████████| 3/3 [00:00<00:00, 1519.86it/s]\nDataset ncbi_disease downloaded and prepared to /root/.cache/huggingface/datasets/ncbi_disease/ncbi_disease/1.0.0/92314c7992b0b8a5ea2ad101be33f365b684a2cc011e0ffa29c691e6d32b2d03. Subsequent calls will reuse this data.\n100%|██████████| 3/3 [00:00<00:00, 177.51it/s]\n","output_type":"stream"},{"output_type":"execute_result","execution_count":1,"data":{"application/vnd.deepnote.dataframe.v3+json":{"column_count":3,"row_count":5,"columns":[{"name":"id","dtype":"object","stats":{"unique_count":5,"nan_count":0,"categories":[{"name":"0","count":1},{"name":"1","count":1},{"name":"3 others","count":3}]}},{"name":"tokens","dtype":"object","stats":{"unique_count":5,"nan_count":0,"categories":[{"name":"['Identification', 'of', 'APC2', ',', 'a', 'homologue', 'of', 'the', 'adenomatous', 'polyposis', 'coli', 'tumour', 'suppressor', '.']","count":1},{"name":"['The', 'adenomatous', 'polyposis', 'coli', '(', 'APC', ')', 'tumour', '-', 'suppressor', 'protein', 'controls', 'the', 'Wnt', 'signalling', 'pathway', 'by', 'forming', 'a', 'complex', 'with', 'glycogen', 'synthase', 'kinase', '3beta', '(', 'GSK', '-', '3beta', ')', ',', 'axin', '/', 'conductin', 'and', 'betacatenin', '.']","count":1},{"name":"3 others","count":3}]}},{"name":"ner_tags","dtype":"object","stats":{"unique_count":5,"nan_count":0,"categories":[{"name":"[0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0, 0]","count":1},{"name":"[0, 1, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]","count":1},{"name":"3 others","count":3}]}},{"name":"_deepnote_index_column","dtype":"int64"}],"rows":[{"id":"0","tokens":"['Identification', 'of', 'APC2', ',', 'a', 'homologue', 'of', 'the', 'adenomatous', 'polyposis', 'coli', 'tumour', 'suppressor', '.']","ner_tags":"[0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0, 0]","_deepnote_index_column":"0"},{"id":"1","tokens":"['The', 'adenomatous', 'polyposis', 'coli', '(', 'APC', ')', 'tumour', '-', 'suppressor', 'protein', 'controls', 'the', 'Wnt', 'signalling', 'pathway', 'by', 'forming', 'a', 'complex', 'with', 'glycogen', 'synthase', 'kinase', '3beta', '(', 'GSK', '-', '3beta', ')', ',', 'axin', '/', 'conductin', 'and', 'betacatenin', '.']","ner_tags":"[0, 1, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]","_deepnote_index_column":"1"},{"id":"2","tokens":"['Complex', 'formation', 'induces', 'the', 'rapid', 'degradation', 'of', 'betacatenin', '.']","ner_tags":"[0, 0, 0, 0, 0, 0, 0, 0, 0]","_deepnote_index_column":"2"},{"id":"3","tokens":"['In', 'colon', 'carcinoma', 'cells', ',', 'loss', 'of', 'APC', 'leads', 'to', 'the', 'accumulation', 'of', 'betacatenin', 'in', 'the', 'nucleus', ',', 'where', 'it', 'binds', 'to', 'and', 'activates', 'the', 'Tcf', '-', '4', 'transcription', 'factor', '(', 'reviewed', 'in', '[', '1', ']', '[', '2', ']', ')', '.']","ner_tags":"[0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]","_deepnote_index_column":"3"},{"id":"4","tokens":"['Here', ',', 'we', 'report', 'the', 'identification', 'and', 'genomic', 'structure', 'of', 'APC', 'homologues', '.']","ner_tags":"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]","_deepnote_index_column":"4"}]},"text/plain":"  id                                             tokens  \\\n0  0  [Identification, of, APC2, ,, a, homologue, of...   \n1  1  [The, adenomatous, polyposis, coli, (, APC, ),...   \n2  2  [Complex, formation, induces, the, rapid, degr...   \n3  3  [In, colon, carcinoma, cells, ,, loss, of, APC...   \n4  4  [Here, ,, we, report, the, identification, and...   \n\n                                            ner_tags  \n0         [0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0, 0]  \n1  [0, 1, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, ...  \n2                        [0, 0, 0, 0, 0, 0, 0, 0, 0]  \n3  [0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n4            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>tokens</th>\n      <th>ner_tags</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>[Identification, of, APC2, ,, a, homologue, of...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0, 0]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>[The, adenomatous, polyposis, coli, (, APC, ),...</td>\n      <td>[0, 1, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>[Complex, formation, induces, the, rapid, degr...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>[In, colon, carcinoma, cells, ,, loss, of, APC...</td>\n      <td>[0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>[Here, ,, we, report, the, identification, and...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom torchtext.data.functional import custom_replace, simple_space_split\nfrom itertools import chain\nfrom nltk.corpus import stopwords\n\nimport nltk\n\n!pip3 install nltk\nnltk.download('stopwords')\n\n\ndef parse_xml(path):\n    with open(path, 'r') as train_corpus: \n        def format_tag(line):\n            all_starts = [idx for idx, item in enumerate(line) if '<passage>' in item]\n            all_ends = [idx for idx, item in enumerate(line) if '</passage>' in item]\n            return [[all_starts[idx], all_ends[idx]] for idx in range(len(all_starts))]\n        data = train_corpus.readlines()\n\n        ind = format_tag(data)\n        return data, ind\n\ndef get_raw_txt(path):\n    raw, indices = parse_xml(path)\n\n    all_entries = [raw[indices[idx][0]+1:indices[idx][1]] for idx in range(len(indices))]\n    all_txt = []\n\n    for x, entry in enumerate(all_entries):\n        empty = []\n        for idx, item in enumerate(entry):\n            if '<text>' in str(item):\n                add_txt = entry[idx].replace('<text>', '')\n                add_txt = add_txt.replace('</text>', '')\n                add_txt = add_txt.replace('\\n', '')\n                empty.append(add_txt)\n            else:\n                pass\n        all_txt.append(''.join(empty))\n    return all_txt\n\ncustom_replace_transform = custom_replace([(r'\\n', ''), (r'<', ''), (r'>', ''), (r'\\t', '')])\npreprocess = lambda x: (simple_space_split(custom_replace_transform(x)))\n\nclass PaperTextDataset(Dataset):\n    def __init__ (self):\n        self.data = {}\n        self.embeddings = []\n        self.length_axis = []\n    \n    def add_dataset(self, name, path, transformation):\n        transformed_data = []\n        for file in os.listdir(path):\n            # try:\n            processed_txt = list(preprocess(transformation(path+'/'+file)))\n            processed_txt = list(chain.from_iterable(processed_txt))\n            processed_txt = [x.lower() for x in processed_txt if (x not in stopwords.words('english')) or (x not in ['a', 'it', 'they', 'I', 'you', 'since', 'because', 'thus', 'therefore', 'however', 'some'])]\n            transformed_data.append(processed_txt[:510])\n            self.length_axis.append(len(processed_txt[:510]))\n            # except:\n                # print(path+file)\n                # break\n\n        # transformed_data = [preprocess(transformation(file)) for file in os.listdir(path)]\n        self.data[name] = transformed_data\n        return f\"Added {name} to dataset.\"\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, name, idx):\n        sample = self.data[name][idx]\n        if self.transform:\n            sample = self.transform(sample)\n        return sample\n\n    def embed(self, tokenizer, model):\n        self.embeddings = []\n        for _, data in self.data.items():\n            for text in data:\n                with torch.no_grad():\n                    inputs = torch.tensor(tokenizer.encode(text)).unsqueeze(0)\n                    outputs = model(inputs)\n                    latent_output = outputs[0]\n                    self.embeddings.append(latent_output)\n        return self.embeddings\n\n'/work/ml/data/nlm-chem-corpus/ALL/5525363_v1.xml'\ndb = PaperTextDataset()\ndb.add_dataset('nlm-chem-corpus', ROOT+'ml/data/nlm-chem-corpus/ALL/', get_raw_txt)","metadata":{"cell_id":"af40c7843d4e4df18265da9bd52ccf5c","source_hash":"11d8bb5c","execution_start":1665810335763,"execution_millis":90758,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Requirement already satisfied: nltk in /shared-libs/python3.9/py/lib/python3.9/site-packages (3.7)\nRequirement already satisfied: regex>=2021.8.3 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from nltk) (2022.9.11)\nRequirement already satisfied: joblib in /shared-libs/python3.9/py/lib/python3.9/site-packages (from nltk) (1.1.0)\nRequirement already satisfied: tqdm in /shared-libs/python3.9/py/lib/python3.9/site-packages (from nltk) (4.64.1)\nRequirement already satisfied: click in /shared-libs/python3.9/py/lib/python3.9/site-packages (from nltk) (8.1.3)\n\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.2.2 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0m[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n","output_type":"stream"},{"output_type":"execute_result","execution_count":21,"data":{"text/plain":"'Added nlm-chem-corpus to dataset.'"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"import json\nimport onnx\nfrom transformers import BertModel, BertConfig, BertTokenizer, BertForPreTraining, load_tf_weights_in_bert\n\n#use biobert main because it's the parameters loaded into the BERT Cased\n#reference for BERT mode: https://github.com/google-research/bert\n\nCONFIG = 'ml/models/biobert_v1.1_pubmed/bert_config.json'\nWEIGHTS = 'ml/models/biobert_v1.1_pubmed/model.ckpt-1000000.index'\nLOAD_MODEL = 'ml/models/biobert.pt'\n\ndef main(config_path, init_checkpoint_path, model_save_path):\n    print('Configuring BERT model parameters ...')\n    config = BertConfig().from_json_file(ROOT+config_path)\n    print('Loading BERT model weights ...')\n    model = BertModel.from_pretrained(ROOT+WEIGHTS, config=config, from_tf=True)\n    print('Saving BioBert Model ...')\n    torch.save(model.state_dict(), ROOT+model_save_path)\n    print('Done!')\n    return model\n\nbiobert = main(CONFIG, WEIGHTS, LOAD_MODEL)\ntokenizer = BertTokenizer.from_pretrained('monologg/biobert_v1.1_pubmed', do_lower_case=True)\nbiobert.eval()","metadata":{"cell_id":"b05935023cbe45eeb4209e7cc8627604","source_hash":"a3baed20","execution_start":1665810531682,"execution_millis":5104,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Configuring BERT model parameters ...\nLoading BERT model weights ...\nSaving BioBert Model ...\nDone!\n","output_type":"stream"},{"output_type":"execute_result","execution_count":22,"data":{"text/plain":"BertModel(\n  (embeddings): BertEmbeddings(\n    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n    (position_embeddings): Embedding(512, 768)\n    (token_type_embeddings): Embedding(2, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): BertEncoder(\n    (layer): ModuleList(\n      (0): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (1): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (2): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (3): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (4): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (5): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (6): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (7): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (8): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (9): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (10): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (11): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (pooler): BertPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n  )\n)"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"len(db.data['nlm-chem-corpus'][0])","metadata":{"tags":[],"cell_id":"51404c2d771b4629b00b29ed039c6cb5","source_hash":"31622456","execution_start":1665810548150,"execution_millis":6,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"execute_result","execution_count":24,"data":{"text/plain":"510"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"import pickle\nnlm_chem_corpus = db.embed(tokenizer=tokenizer, model=biobert)\n\nwith open(ROOT+'ml/models/embeddings_biobert', 'wb') as output_file:\n    pickle.dump(nlm_chem_corpus, output_file)\n\nwith open(ROOT+\"ml/models/embeddings_biobert\", \"rb\") as input_file:\n    nlm_embeddings = pickle.load(input_file)","metadata":{"tags":[],"cell_id":"e4ed1ca0f44b4665b51367f74a42a031","source_hash":"6aa85760","execution_start":1665810550073,"execution_millis":423861,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":25},{"cell_type":"code","source":"print(nlm_chem_corpus[1].shape)","metadata":{"tags":[],"cell_id":"9a484dc9e2f942699bd62eca315483af","source_hash":"7b671b47","execution_start":1665810986389,"execution_millis":4,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"torch.Size([1, 512, 768])\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# print(nlm_embeddings[0][0])\n# print(nlm_chem_corpus[0][0])\n\narray_nlp_embeddings = np.asarray([x.detach().numpy() for x in nlm_embeddings])\nprint(array_nlp_embeddings[0])","metadata":{"tags":[],"cell_id":"c97d4e2ba03149b0a7662f66e6598462","source_hash":"f4e7481f","execution_start":1665811002656,"execution_millis":480,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"[[[-1.0685854e-03 -2.3804055e-01 -3.3589444e-01 ... -3.6289218e-01\n   -1.9453727e-01  1.3101462e-01]\n  [ 6.8597639e-01 -1.8458228e-01  2.0089817e-01 ... -3.0724707e-01\n   -2.4389689e-01  3.9504102e-01]\n  [ 1.1524965e+00 -3.2597321e-01 -1.1474037e-01 ... -1.2518139e-01\n    4.6430829e-01  1.2401047e-02]\n  ...\n  [ 6.5270627e-01 -4.6341980e-01 -1.4489470e-01 ...  1.0597266e+00\n   -4.7618228e-01 -1.1120352e-01]\n  [ 5.8118361e-01 -4.4527462e-01 -1.0299656e+00 ... -4.0091872e-02\n   -5.2802771e-01  1.0137630e-02]\n  [ 1.8052539e-02  3.1354332e-01 -1.4269681e-01 ... -1.0099059e-01\n   -8.4040093e-01 -4.3762574e-01]]]\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=05362208-35d3-4fa0-ada7-4013d10444bf' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"tags":[],"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"vscode":{"interpreter":{"hash":"577ea14e5a220a2e2ceb4e7a6eabde6a6424f50ee02cc5f86f913307280e145a"}},"deepnote":{},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3.8.12 ('machine_learning')"},"language_info":{"name":"python","version":"3.8.12","mimetype":"text/x-python","file_extension":".py","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python"},"orig_nbformat":4,"deepnote_notebook_id":"1fe2aabc4a62476bb4b97ec34ed23ae5","deepnote_execution_queue":[],"deepnote_persisted_session":{"createdAt":"2022-10-23T18:31:52.336Z"}}}